{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_from_disk(\"posts/combined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<p>Could someone please point me in the right direction with respect to OHLC data timeframe conversion with <a href=\"http://pandas.pydata.org/\" rel=\"noreferrer\">Pandas</a>? What I\\'m trying to do is build a Dataframe with data for higher timeframes, given data with lower timeframe. </p>\\n\\n<p>For example, given I have the following one-minute (M1) data:</p>\\n\\n<pre><code>                       Open    High     Low   Close  Volume\\nDate                                                       \\n1999-01-04 10:22:00  1.1801  1.1819  1.1801  1.1817       4\\n1999-01-04 10:23:00  1.1817  1.1818  1.1804  1.1814      18\\n1999-01-04 10:24:00  1.1817  1.1817  1.1802  1.1806      12\\n1999-01-04 10:25:00  1.1807  1.1815  1.1795  1.1808      26\\n1999-01-04 10:26:00  1.1803  1.1806  1.1790  1.1806       4\\n1999-01-04 10:27:00  1.1801  1.1801  1.1779  1.1786      23\\n1999-01-04 10:28:00  1.1795  1.1801  1.1776  1.1788      28\\n1999-01-04 10:29:00  1.1793  1.1795  1.1782  1.1789      10\\n1999-01-04 10:31:00  1.1780  1.1792  1.1776  1.1792      12\\n1999-01-04 10:32:00  1.1788  1.1792  1.1788  1.1791       4\\n</code></pre>\\n\\n<p>which has Open, High, Low, Close (OHLC) and volume values for every minute I would like to build a set of 5-minute readings (M5) which would look like so:</p>\\n\\n<pre><code>                       Open    High     Low   Close  Volume\\nDate                                                       \\n1999-01-04 10:25:00  1.1807  1.1815  1.1776  1.1789      91\\n1999-01-04 10:30:00  1.1780  1.1792  1.1776  1.1791      16\\n</code></pre>\\n\\n<p>So the workflow is that:</p>\\n\\n<ul>\\n<li>Open is the Open of the first row in the timewindow</li>\\n<li>High is the highest High in the timewindow</li>\\n<li>Low is the lowest Low</li>\\n<li>Close is the last Close</li>\\n<li>Volume is simply a sum of Volumes</li>\\n</ul>\\n\\n<p>There are few issues though:</p>\\n\\n<ul>\\n<li>the data has gaps ( note there is no 10:30:00 row)</li>\\n<li>the 5-minute intervals have to start at round time, e.g. M5 starts at 10:25:00 not 10:22:00</li>\\n<li>first, incomplete set can be omitted like in this example, or included (so we could have 10:20:00 5-minute entry)</li>\\n</ul>\\n\\n<p>The <a href=\"http://pandas.sourceforge.net/timeseries.html#up-and-downsampling\" rel=\"noreferrer\">Pandas documentation on up-down sampling</a> gives an example, but they use mean value as the value of up-sampled row, which won\\'t work here. I have tried using <code>groupby</code> and <code>agg</code> but to no avail. For one getting highest High and lowest Low might be not so hard, but I have no idea how to get first Open and last Close.</p>\\n\\n<p>What I tried is something along the lines of:</p>\\n\\n<pre><code>grouped = slice.groupby( dr5minute.asof ).agg( \\n    { \\'Low\\': lambda x : x.min()[ \\'Low\\' ], \\'High\\': lambda x : x.max()[ \\'High\\' ] } \\n)\\n</code></pre>\\n\\n<p>but it results in following error, which I don\\'t understand:</p>\\n\\n<pre><code>In [27]: grouped = slice.groupby( dr5minute.asof ).agg( { \\'Low\\' : lambda x : x.min()[ \\'Low\\' ], \\'High\\' : lambda x : x.max()[ \\'High\\' ] } )\\n---------------------------------------------------------------------------\\nIndexError                                Traceback (most recent call last)\\n/work/python/fxcruncher/&lt;ipython-input-27-df50f9522a2f&gt; in &lt;module&gt;()\\n----&gt; 1 grouped = slice.groupby( dr5minute.asof ).agg( { \\'Low\\' : lambda x : x.min()[ \\'Low\\' ], \\'High\\' : lambda x : x.max()[ \\'High\\' ] } )\\n\\n/usr/lib/python2.7/site-packages/pandas/core/groupby.pyc in agg(self, func, *args, **kwargs)\\n    242         See docstring for aggregate\\n    243         \"\"\"\\n--&gt; 244         return self.aggregate(func, *args, **kwargs)\\n    245 \\n    246     def _iterate_slices(self):\\n\\n/usr/lib/python2.7/site-packages/pandas/core/groupby.pyc in aggregate(self, arg, *args, **kwargs)\\n   1153                     colg = SeriesGroupBy(obj[col], column=col,\\n   1154                                          grouper=self.grouper)\\n-&gt; 1155                     result[col] = colg.aggregate(func)\\n   1156 \\n   1157             result = DataFrame(result)\\n\\n/usr/lib/python2.7/site-packages/pandas/core/groupby.pyc in aggregate(self, func_or_funcs, *args, **kwargs)\\n    906                 return self._python_agg_general(func_or_funcs, *args, **kwargs)\\n    907             except Exception:\\n--&gt; 908                 result = self._aggregate_named(func_or_funcs, *args, **kwargs)\\n    909 \\n    910             index = Index(sorted(result), name=self.grouper.names[0])\\n\\n/usr/lib/python2.7/site-packages/pandas/core/groupby.pyc in _aggregate_named(self, func, *args, **kwargs)\\n    976             grp = self.get_group(name)\\n    977             grp.name = name\\n--&gt; 978             output = func(grp, *args, **kwargs)\\n    979             if isinstance(output, np.ndarray):\\n    980                 raise Exception(\\'Must produce aggregated value\\')\\n\\n/work/python/fxcruncher/&lt;ipython-input-27-df50f9522a2f&gt; in &lt;lambda&gt;(x)\\n----&gt; 1 grouped = slice.groupby( dr5minute.asof ).agg( { \\'Low\\' : lambda x : x.min()[ \\'Low\\' ], \\'High\\' : lambda x : x.max()[ \\'High\\' ] } )\\n\\nIndexError: invalid index to scalar variable.\\n</code></pre>\\n\\n<p>So any help on doing that would be greatly appreciated. If the path I chose is not going to work, please suggest other relatively efficient approach (I have millions of rows). Some resources on using Pandas for financial processing would also be nice.</p>\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = ds['Body'][55]\n",
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = ET.fromstring(\"<body>\" + example + \"</body>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could someone please point me in the right direction with respect to OHLC data timeframe conversion with [Pandas](http://pandas.pydata.org/)\n",
      "\n",
      "For example, given I have the following one-minute (M1) data:\n",
      "\n",
      "```\n",
      "                       Open    High     Low   Close  Volume\n",
      "Date                                                       \n",
      "1999-01-04 10:22:00  1.1801  1.1819  1.1801  1.1817       4\n",
      "1999-01-04 10:23:00  1.1817  1.1818  1.1804  1.1814      18\n",
      "1999-01-04 10:24:00  1.1817  1.1817  1.1802  1.1806      12\n",
      "1999-01-04 10:25:00  1.1807  1.1815  1.1795  1.1808      26\n",
      "1999-01-04 10:26:00  1.1803  1.1806  1.1790  1.1806       4\n",
      "1999-01-04 10:27:00  1.1801  1.1801  1.1779  1.1786      23\n",
      "1999-01-04 10:28:00  1.1795  1.1801  1.1776  1.1788      28\n",
      "1999-01-04 10:29:00  1.1793  1.1795  1.1782  1.1789      10\n",
      "1999-01-04 10:31:00  1.1780  1.1792  1.1776  1.1792      12\n",
      "1999-01-04 10:32:00  1.1788  1.1792  1.1788  1.1791       4\n",
      "\n",
      "```\n",
      "\n",
      "which has Open, High, Low, Close (OHLC) and volume values for every minute I would like to build a set of 5-minute readings (M5) which would look like so:\n",
      "\n",
      "```\n",
      "                       Open    High     Low   Close  Volume\n",
      "Date                                                       \n",
      "1999-01-04 10:25:00  1.1807  1.1815  1.1776  1.1789      91\n",
      "1999-01-04 10:30:00  1.1780  1.1792  1.1776  1.1791      16\n",
      "\n",
      "```\n",
      "\n",
      "So the workflow is that:\n",
      "\n",
      "\n",
      "- Open is the Open of the first row in the timewindow\n",
      "- High is the highest High in the timewindow\n",
      "- Low is the lowest Low\n",
      "- Close is the last Close\n",
      "- Volume is simply a sum of Volumes\n",
      "\n",
      "\n",
      "There are few issues though:\n",
      "\n",
      "\n",
      "- the data has gaps ( note there is no 10:30:00 row)\n",
      "- the 5-minute intervals have to start at round time, e.g. M5 starts at 10:25:00 not 10:22:00\n",
      "- first, incomplete set can be omitted like in this example, or included (so we could have 10:20:00 5-minute entry)\n",
      "\n",
      "\n",
      "The [Pandas documentation on up-down sampling](http://pandas.sourceforge.net/timeseries.html#up-and-downsampling)`groupby``agg`\n",
      "\n",
      "What I tried is something along the lines of:\n",
      "\n",
      "```\n",
      "grouped = slice.groupby( dr5minute.asof ).agg( \n",
      "    { 'Low': lambda x : x.min()[ 'Low' ], 'High': lambda x : x.max()[ 'High' ] } \n",
      ")\n",
      "\n",
      "```\n",
      "\n",
      "but it results in following error, which I don't understand:\n",
      "\n",
      "```\n",
      "In [27]: grouped = slice.groupby( dr5minute.asof ).agg( { 'Low' : lambda x : x.min()[ 'Low' ], 'High' : lambda x : x.max()[ 'High' ] } )\n",
      "---------------------------------------------------------------------------\n",
      "IndexError                                Traceback (most recent call last)\n",
      "/work/python/fxcruncher/<ipython-input-27-df50f9522a2f> in <module>()\n",
      "----> 1 grouped = slice.groupby( dr5minute.asof ).agg( { 'Low' : lambda x : x.min()[ 'Low' ], 'High' : lambda x : x.max()[ 'High' ] } )\n",
      "\n",
      "/usr/lib/python2.7/site-packages/pandas/core/groupby.pyc in agg(self, func, *args, **kwargs)\n",
      "    242         See docstring for aggregate\n",
      "    243         \"\"\"\n",
      "--> 244         return self.aggregate(func, *args, **kwargs)\n",
      "    245 \n",
      "    246     def _iterate_slices(self):\n",
      "\n",
      "/usr/lib/python2.7/site-packages/pandas/core/groupby.pyc in aggregate(self, arg, *args, **kwargs)\n",
      "   1153                     colg = SeriesGroupBy(obj[col], column=col,\n",
      "   1154                                          grouper=self.grouper)\n",
      "-> 1155                     result[col] = colg.aggregate(func)\n",
      "   1156 \n",
      "   1157             result = DataFrame(result)\n",
      "\n",
      "/usr/lib/python2.7/site-packages/pandas/core/groupby.pyc in aggregate(self, func_or_funcs, *args, **kwargs)\n",
      "    906                 return self._python_agg_general(func_or_funcs, *args, **kwargs)\n",
      "    907             except Exception:\n",
      "--> 908                 result = self._aggregate_named(func_or_funcs, *args, **kwargs)\n",
      "    909 \n",
      "    910             index = Index(sorted(result), name=self.grouper.names[0])\n",
      "\n",
      "/usr/lib/python2.7/site-packages/pandas/core/groupby.pyc in _aggregate_named(self, func, *args, **kwargs)\n",
      "    976             grp = self.get_group(name)\n",
      "    977             grp.name = name\n",
      "--> 978             output = func(grp, *args, **kwargs)\n",
      "    979             if isinstance(output, np.ndarray):\n",
      "    980                 raise Exception('Must produce aggregated value')\n",
      "\n",
      "/work/python/fxcruncher/<ipython-input-27-df50f9522a2f> in <lambda>(x)\n",
      "----> 1 grouped = slice.groupby( dr5minute.asof ).agg( { 'Low' : lambda x : x.min()[ 'Low' ], 'High' : lambda x : x.max()[ 'High' ] } )\n",
      "\n",
      "IndexError: invalid index to scalar variable.\n",
      "\n",
      "```\n",
      "\n",
      "So any help on doing that would be greatly appreciated. If the path I chose is not going to work, please suggest other relatively efficient approach (I have millions of rows). Some resources on using Pandas for financial processing would also be nice.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def traverse(node):\n",
    "    traverse_children = False\n",
    "    text = \"\"\n",
    "    match node.tag:\n",
    "        case \"a\":\n",
    "            return f\"[{node.text}]({node.attrib['href']})\"\n",
    "        case \"h1\":\n",
    "            return f\"\\n# {node.text}\\n\\n\"\n",
    "        case \"h2\":\n",
    "            return f\"\\n## {node.text}\\n\\n\"\n",
    "        case \"h3\":\n",
    "            return f\"\\n### {node.text}\\n\\n\"\n",
    "        case \"h4\":\n",
    "            return f\"\\n#### {node.text}\\n\\n\"\n",
    "        case \"h5\":\n",
    "            return f\"\\n##### {node.text}\\n\\n\"\n",
    "        case \"h6\":\n",
    "            return f\"\\n###### {node.text}\\n\\n\"\n",
    "        case \"code\":\n",
    "            return f\"`{node.text}`\"\n",
    "        case \"blockquote\":\n",
    "            text = f\"> \"\n",
    "            traverse_children = True\n",
    "        case \"hr\":\n",
    "            return f\"\\n---\\n\\n\"\n",
    "        case \"img\":\n",
    "            return f\"![{node.attrib['alt']}]({node.attrib['src']})\"\n",
    "        case \"li\":\n",
    "            text = f\"- {node.text}\"\n",
    "            traverse_children = True\n",
    "        case \"pre\":\n",
    "            has_code = False\n",
    "            for child in node:\n",
    "                if child.tag == \"code\":\n",
    "                    has_code = True\n",
    "                    text = f\"```\\n{child.text}\\n```\"\n",
    "            if not has_code:\n",
    "                traverse_children = True\n",
    "                if node.text:\n",
    "                    text = node.text + text\n",
    "        case other:\n",
    "            traverse_children = True\n",
    "            if node.text:\n",
    "                text = node.text + text\n",
    "    if traverse_children:\n",
    "        for child in node:\n",
    "            text += traverse(child)\n",
    "    if node.tail:\n",
    "        text += node.tail\n",
    "    return text\n",
    "\n",
    "print(traverse(root))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
